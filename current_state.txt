composition, interactions, déroulement, résolution de problèmes de toute nature et tout élément pertinent sur le travail d’équipe. 
Pour l’option 2, la conception doit être présentée clairement Pour la partie le codage remettre les codes sources, compilés avec une documentation d’utilisation. Rajouter au  fichier Word (conception)   les résultats des tests. Pour chaque option une liste de références est nécessaire avec des citations au niveau du texte et l’intégration à la liste à présenter a la fin du document. Le rapport doit être bien structuré avec des titres significatifs et des illustrations incluant des captures écran. Le rapport ne doit pas dépasser 15 pages pour les deux options et au besoin utiliser une annexe pour le code ou les illustrations

Le projet utilise CARLA comme environnement pour l'agent.

Initialement on pensait que l'interaction avec le serveur serait simple.

Le premier problème rencontré avait avoir avec l'utilisation de CARLA parce que customiser les cartes sur lesquelles faire pratiquer notre agent n'était pas simple.

Notre solution est un mélange de cartes généré par l'entremise du standard openDRIVE et l'utiliation des cartes par défaut pour la validation de l'apprentissage.

L'implantation d'un agent apprenant dans CARLA est complexe à cause de la difficulté qui existe à rendre les données du simulateur compatible avec un agent créé avec notre module de machine learning de choix (PyTorch).

Notre solution tentative implique utiliser le module gymnasium comme wrapper pour les données liés au simulateur et en suite les passer au code d'apprentissage.

Je comptais utiliser le lidar pour obtenir la distance vers les obstacles mais le fait qu'il doit tourner pour avoir une bonne détection des alentours veux dire qu'il est plus difficile pour l'agent de faire du sens de ses données si elle sont réduites en une distance relative.

Les problemes rencontrés avec le radar ont principalement a voir avec bien définir ou il pointe ce qui à demandé l'ajout d'un outil pour visionné ce qui voit le véhicule.

Tester les capteurs de l'agent c'est fait à l'aide de code pour pourvoir observer ou le radar pointe et les données qu'il retourne.

Ces données sont aussi présenté dans le dashboard de l'enviromment qui l'affiche sous forme de carte top down avec les lignes généré par le radar.

Compte-tenu que notre but est seulemen de faire l'évitement d'autre capteurs ne sont pas nécessaires

Je suis retourné au capteur liDAR parce qu'il offre une meilleur précision dans ces observations
Ses données sont transformé en un liste de points boolean qui représente true si il n'y a rien et false si il y a un obstacle


Comme il est très difficile pour moi de voir comment discretisé l'état de ma simulation, j'ai décidé d'utiliser une version de deep learning pour le moment.
Deep q learning fait l'affaire parce que discretisé les actions possibles est très simple.

04-01 Mon problème actuel est de faire fonctionné ensemble toute les couches de mon réseau neuronal.
Solutions tentatives:
Changer les couches précédents la couche problématique (ceci requiert modifier les données entrants de mon réseaux)
Faire plus de modification intermédiaires sur les données
Le but du réseau est de finir avec une valeur entière entre 0 et 4

le fichier test contient l'implantation de deep q learning la plus à jour.

Il y a beaucoup de nombres a ajusté et beaucoup de modifications à faire dans le code de l'agent

Known issues:
l'optimisateur de model cause des erreurs à cause de changement de taille
hypothesis 1 : Error related to reset causing speed too reach out of bounds speeds

BATCH_SIZE must be a multiple of 508 because that is th return size of the neural net

Ajout de l'implantation de PPO implanter avec F1tenth

Donner plus de temps au système pour découvrir quoi faire par épisode serait bénéfique

La fonction d'update uilisé peut être améliorer avec l'usage de torch.optim

Resultats:
Run 1: 4096 float input size in 1h45min the car couldn't dodge obstacles with any kind of consistency but you could see it trying to avoid going into the wall until it gave up
Run 2: 1026 float input size in 20min the car seems to dodge obstacles it can see. It crashes into the slide because a 2d slice of the map doesn't allow for good enough detection in a 32x32 grid (the only solution is giving it more data but that slows training considerably) 
       otherwise giving it supplemental data from other more sensitive sensor could remedy this issue.
Run 3: Giving more points for speeding up (by making speed affect reward) it tries to avoid walls and doesn't seem affect by the added point gain of speed (error: it gained 50 every frame if the speed was below 50 km/h)
Run 4: Reducing the fov to 180 degrees ahead of the car(514 floats for 32x32 screen size) (observations: the field of data given to the model can now be safely cut in half | initially the car's wheels seem to follow the lidar ray)
Run 5: Making the data update as a sweep (radar style only update when scan passes) for more stable sensor data and cutting the data range in half 
Run 6: Ajustments(added base survival reward, reduce reward for speed) ran into the wall more deliberately
Run 7: no change from run 6 except bug fixing an issue with the sensor data
Run 8: (change: Remove base survival) reward it avoid crashing more
Run 9: (change: remove time limit on episodes)
Run 10: (change: increase limit to 10000 steps an episode and adding punishment for no movement)
Run 11: Truncate sensor values even more (change 546 input values)
Run 12: Truncate sensor values even more (change 92 input values)
Run 12: Truncate sensor values even more and up the resolution of the input (change 134 input values) runs into walls
Run 13: Removing sensor radar-esque scanning behaviour
Run 15: Update policy quicker
Run 16: Extend the field of sensor (input size: 277) to make outline more noticeable
Run 18: Make negative reward for collision relative to all actions taken and Reduce episode length the length gives it the time to learn bad habits early on it can't get rid of (result: 3h still doesn't consistently dodge walls)
Run 20: Back to updating the policy once every 4 episodes and reduce points per scan to 10 (failed)
Run 21: Slow down scan speed
Run 22: Speed up scan add functional scan lidar (the scan refresh rate is to slow)
Run 23: Add decay to scan data
Run 24: decay 4x a cycle instead of once ( Result: Seems to work but has to get very close to obstacles before it does anything )
Run 25: Expand range back to 50
Idea: modify model to add vec2d for lidar data that is then combined with the other 2 float values
         vec2d to 1x4 float + 1x2 float 

documentation
https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html?highlight=deep%20q
https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py
https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html
https://github.com/bitsauce/Carla-ppo
https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/PPO/torch
https://github.com/ericyangyu/PPO-for-Beginners
https://github.com/AndersonJo/dqn-pytorch