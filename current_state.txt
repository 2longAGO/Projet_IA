composition, interactions, déroulement, résolution de problèmes de toute nature et tout élément pertinent sur le travail d’équipe. 
Pour l’option 2, la conception doit être présentée clairement Pour la partie le codage remettre les codes sources, compilés avec une documentation d’utilisation. Rajouter au  fichier Word (conception)   les résultats des tests. Pour chaque option une liste de références est nécessaire avec des citations au niveau du texte et l’intégration à la liste à présenter a la fin du document. Le rapport doit être bien structuré avec des titres significatifs et des illustrations incluant des captures écran. Le rapport ne doit pas dépasser 15 pages pour les deux options et au besoin utiliser une annexe pour le code ou les illustrations

Le projet utilise CARLA comme environnement pour l'agent.

Initialement on pensait que l'interaction avec le serveur serait simple.

Le premier problème rencontré avait avoir avec l'utilisation de CARLA parce que customiser les cartes sur lesquelles faire pratiquer notre agent n'était pas simple.

Notre solution est un mélange de cartes généré par l'entremise du standard openDRIVE et l'utiliation des cartes par défaut pour la validation de l'apprentissage.

L'implantation d'un agent apprenant dans CARLA est complexe à cause de la difficulté qui existe à rendre les données du simulateur compatible avec un agent créé avec notre module de machine learning de choix (PyTorch).

Notre solution tentative implique utiliser le module gymnasium comme wrapper pour les données liés au simulateur et en suite les passer au code d'apprentissage.

Je comptais utiliser le lidar pour obtenir la distance vers les obstacles mais le fait qu'il doit tourner pour avoir une bonne détection des alentours veux dire qu'il est plus difficile pour l'agent de faire du sens de ses données si elle sont réduites en une distance relative.

Les problemes rencontrés avec le radar ont principalement a voir avec bien définir ou il pointe ce qui à demandé l'ajout d'un outil pour visionné ce qui voit le véhicule.

Tester les capteurs de l'agent c'est fait à l'aide de code pour pourvoir observer ou le radar pointe et les données qu'il retourne.

Ces données sont aussi présenté dans le dashboard de l'enviromment qui l'affiche sous forme de carte top down avec les lignes généré par le radar.

Compte-tenu que notre but est seulemen de faire l'évitement d'autre capteurs ne sont pas nécessaires

Je suis retourné au capteur liDAR parce qu'il offre une meilleur précision dans ces observations
Ses données sont transformé en un liste de points boolean qui représente true si il n'y a rien et false si il y a un obstacle


Comme il est très difficile pour moi de voir comment discretisé l'état de ma simulation, j'ai décidé d'utiliser une version de deep learning pour le moment.
Deep q learning fait l'affaire parce que discretisé les actions possibles est très simple.

https://github.com/AndersonJo/dqn-pytorch (inspiration pour la configuration du réseau neuronal)
04-01 Mon problème actuel est de faire fonctionné ensemble toute les couches de mon réseau neuronal.
Solutions tentatives:
Changer les couches précédents la couche problématique (ceci requiert modifier les données entrants de mon réseaux)
Faire plus de modification intermédiaires sur les données
Le but du réseau est de finir avec une valeur entière entre 0 et 4

le fichier test contient l'implantation de deep q learning la plus à jour.

Il y a beaucoup de nombres a ajusté et beaucoup de modifications à faire dans le code de l'agent

Known issues:
l'optimisateur de model cause des erreurs à cause de changement de taille
hypothesis 1 : Error related to reset causing speed too reach out of bounds speeds

BATCH_SIZE must be a multiple of 508 because that is th return size of the neural net

Nous avons décidé d'utiliser f1tenth pour simplifier la portion environnemnt du projet 
Les  observations et actions sont à peu près identiques à l'ancien environnement lidar pour la vue du circuit et la vitesse du véhicule et les action sont 2 plage de valeurs flottantes qui représente la pédal de gas et de freinage.

PPO implementation https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py

This PPO implementation could probably also be used in with he crla environment

Le fichier train_projet est utilisé pour faire l'entrainement de l'IA

Run 4: Avant l'ajout des règles de reward l'AI en 322 episodes a arrêter de bouger.
Run 5: Ajout de la règle qui demande une certaine vitesse pour obtenir des points vitesse > 1
Il a arrêter de bouger parce que le threshold de vitesse était trop élevé pour qu'il se rende compte du but
Run 6: Rêgle de vitesse basé sur la multiplication de la reward par la vitesse
    la rêgle ne discrimine pas contre le reculons
Run 7: Retirer l'habileté de reculer
Run 8: Diminuer la plage d' actions et diminuer les lasers pris en compte à 16. (Résultats: Semble éviter activement les collisions après juste 53 épisodes)
Run 9: Augmenter la valeur de la vitesse
Run 10: Augmenter la durée d'une épisode
Run 11: Diminuer la durée d'une épisode
Run 12: Back to all lidar values minus backwards
Run 12: reduce radars and give less penalty for collisions
Run 13: reduce input data to 10
Test to see if the removal of all backwards is a detriment
Run 14: (log 23) Increase learning rate value of the critic (0.001) (Seems less finicky in it's decisions)
Run 15: Adjust sensor data (It figured out that point can be gained from passing a specific line)
Run 16: Increase learning rate of actor (0.0008)

Run ppo f110: scans rays 17 for track: TRACK_1 150000 steps
Run ppo f110_1: scans rays 17 for track: OBSTACLES 150000 steps
Run ppo f110_2: scans rays 17 for track: OBSTACLES  200000 steps
Run ppo f110_3: scans rays 65 for trck: OBSTACLES 150000 steps
Increase agent speed backup 
(TO-DO) Add incentive to go forward
Idea: return an array of nbRay size with only the longest rays not at 0 and the agent would only need to learn to follow the direction that ray indicates
Observation génerale la performance est toujours meilleur avec peu de données d'entrée

https://github.com/johschmitz/blender-driving-scenario-creator
https://stable-baselines3.readthedocs.io/en/master/index.html